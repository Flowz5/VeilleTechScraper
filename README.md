
# ğŸ›¡ï¸ Cyber-Watch : Veille Technologique & Cyber-Intelligence

![Python](https://img.shields.io/badge/Python-3.11%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![MySQL](https://img.shields.io/badge/MySQL-4479A1?style=for-the-badge&logo=mysql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Container-blue?style=for-the-badge&logo=docker&logoColor=white)
![n8n](https://img.shields.io/badge/n8n-Automation-EA4B71?style=for-the-badge&logo=n8n&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

**Cyber-Watch** est une plateforme de veille technologique automatisÃ©e et conteneurisÃ©e, orientÃ©e CybersÃ©curitÃ© et DevOps.

Au-delÃ  de l'agrÃ©gation RSS, le projet intÃ¨gre un pipeline **ETL** (Extract, Transform, Load) complet : collecte Python, stockage SQL, analyse sÃ©mantique (Scoring) et **orchestration d'alertes en temps rÃ©el via n8n et Discord**.

---

## ğŸš€ Architecture Technique

Le systÃ¨me repose sur 4 piliers interconnectÃ©s :

1.  **Collecte (Python)** : Scraper multi-threadÃ© qui interroge ~20 sources (ANSSI, BleepingComputer, etc.).
2.  **Intelligence (Algo)** : Moteur de scoring qui dÃ©tecte les mots-clÃ©s critiques (*Ransomware, CVE, 0-Day*).
3.  **Stockage (MySQL)** : Base de donnÃ©es relationnelle avec gestion d'unicitÃ©.
4.  **Notification (Docker + n8n)** : Envoi automatique d'alertes formatÃ©es sur Discord pour les menaces critiques (Score â‰¥ 4).

---

## âš™ï¸ FonctionnalitÃ©s ClÃ©s

### 1. Collecte & Filtrage (`scraper.py`)
* **Hybride** : Gestion des flux RSS et parsing HTML (BeautifulSoup).
* **Robustesse** : Contournement des protections (User-Agents, SSL) pour les sites gouvernementaux.
* **DÃ©duplication** : VÃ©rification en base avant insertion.

### 2. Moteur de Pertinence (Scoring)
Chaque article reÃ§oit un score de **0 Ã  10** selon sa criticitÃ© :
* ğŸ”´ **Critique (+3 pts)** : *Ransomware, 0-day, Breach, CVE, Faille...* -> **DÃ©clenche une alerte Discord**.
* ğŸŸ  **Important (+2 pts)** : *ANSSI, GDPR, Python, Docker...*
* ğŸ”µ **Contexte (+1 pt)** : *Windows, Update, Web, Tech...*

### 3. Automatisation & Alerting (`n8n`)
Un conteneur Docker **n8n** Ã©coute les webhooks envoyÃ©s par le script Python.
* RÃ©ception des donnÃ©es (Titre, Source, Lien, Score).
* Formatage du message (Rich text).
* Dispatch vers un channel Discord dÃ©diÃ© Ã  la veille.

### 4. Dashboard BI (`dashboard.py`)
Interface Streamlit pour la consultation Ã  froid :
* Fil d'actualitÃ© priorisÃ© par score.
* Nuage de mots-clÃ©s dynamique (WordCloud).
* Statistiques sur les sources les plus actives.

---

## ğŸ“ Structure du Projet

```bash
VeilleTechScraper/
â”œâ”€â”€ scraper.py           # Backend : Collecte, Scoring, Webhook vers n8n
â”œâ”€â”€ dashboard.py         # Frontend : Interface Streamlit & Dataviz
â”œâ”€â”€ n8n_automation.json  # Workflow d'automatisation (Import n8n)
â”œâ”€â”€ requirements.txt     # DÃ©pendances Python
â”œâ”€â”€ .env                 # Secrets (DB, etc.)
â””â”€â”€ README.md            # Documentation

```

---

## ğŸ› ï¸ Installation & DÃ©marrage

### 1. PrÃ©requis

* Python 3.10+
* Docker & Docker Compose
* Serveur MySQL

### 2. Installation Python & BDD

```bash
git clone [https://github.com/Flowz5/VeilleTechScraper.git](https://github.com/Flowz5/VeilleTechScraper.git)
cd VeilleTechScraper

# Environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac

# DÃ©pendances
pip install -r requirements.txt

```

CrÃ©ez un fichier `.env` Ã  la racine :

```ini
DB_HOST=localhost
DB_USER=root
DB_PASSWORD=votre_mot_de_passe
DB_NAME=veille_tech
# (L'URL n8n est configurÃ©e dans le script scraper.py)

```

### 3. Mise en place de l'Automatisation (n8n)

Lancer le conteneur n8n (avec persistance des donnÃ©es) :

```bash
docker run -d \
  --name n8n \
  -p 5678:5678 \
  -v ~/.n8n:/home/node/.n8n:Z \
  --restart always \
  docker.n8n.io/n8nio/n8n

```

**Configuration du Workflow :**

1. AccÃ©der Ã  `http://localhost:5678`.
2. Cliquer sur **"Add workflow"** > **"Import from..."** > **"File"**.
3. SÃ©lectionner le fichier `n8n_automation.json` prÃ©sent dans ce dÃ©pÃ´t.
4. Double-cliquer sur le nÅ“ud **Discord**.
5. Dans "Credential", crÃ©ez un nouveau "Discord Webhook account" et collez **votre propre URL de Webhook Discord**.
6. **Activez** le workflow (Bouton en haut Ã  droite).

### 4. Utilisation

**Mode Manuel :**

```bash
python scraper.py        # RÃ©cupÃ¨re les articles et alerte si critique
streamlit run dashboard.py # Lance l'interface visuelle

```

**Mode Automatique (Cron) :**
Ajouter au crontab pour une exÃ©cution tous les matins Ã  8h00 :

```bash
0 8 * * * /chemin/vers/venv/bin/python /chemin/vers/scraper.py >> /chemin/vers/cron.log 2>&1

```

---

## ğŸ§ IntÃ©gration Linux (Alias)

Pour les utilisateurs avancÃ©s (Hyprland / Bash / Zsh) :

```bash
alias cyberwatch="cd ~/Projets/VeilleTechScraper && source venv/bin/activate && streamlit run dashboard.py"
alias cyberscrape="cd ~/Projets/VeilleTechScraper && source venv/bin/activate && python scraper.py"

```
