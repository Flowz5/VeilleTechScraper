import os
import requests
from bs4 import BeautifulSoup
import datetime
import mysql.connector
import sys
import logging
from dotenv import load_dotenv

# --- IMPORTS RICH ---
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.panel import Panel
from rich.theme import Theme

# Configuration du th√®me Rich
custom_theme = Theme({"success": "green", "warning": "yellow", "error": "bold red", "info": "cyan"})
console = Console(theme=custom_theme)

# Charger les variables
load_dotenv()

# --- CONFIGURATION LOGGING ---
logging.basicConfig(
    filename='journal.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# --- CONFIGURATION SOURCES ---
SOURCES = {
    # --- CYBERS√âCURIT√â (FR) ---
    "[CYBER] ANSSI (CERT-FR)": "https://www.cert.ssi.gouv.fr/feed/",
    "[CYBER] Le Monde Informatique": "https://www.lemondeinformatique.fr/flux-rss/rubrique/cybersecurite/rss.xml",
    "[CYBER] Zataz": "https://www.zataz.com/feed/",
    "[CYBER] ZDNet S√©cu": "https://www.zdnet.fr/feeds/rss/actualites/security/",
    "[CYBER] WeLiveSecurity (ESET)": "https://www.welivesecurity.com/fr/feed/",
    
    # --- CYBERS√âCURIT√â (US - Indispensable pour la r√©activit√©) ---
    "[CYBER üá∫üá∏] The Hacker News": "https://feeds.feedburner.com/TheHackersNews",
    "[CYBER üá∫üá∏] BleepingComputer": "https://www.bleepingcomputer.com/feed/",
    "[CYBER üá∫üá∏] Google Security Blog": "https://security.googleblog.com/feeds/posts/default",
    
    # --- D√âVELOPPEMENT & PYTHON ---
    "[DEV] Developpez.com": "https://www.developpez.com/index/rss",
    "[DEV] Journal du Hacker": "https://www.journalduhacker.net/rss",
    "[DEV] GitHub Blog": "https://github.blog/feed/",
    "[DEV üá∫üá∏] Real Python": "https://realpython.com/atom.xml",
    "[DEV üá∫üá∏] Dev.to": "https://dev.to/feed",
    
    # --- INFRA, LINUX & CLOUD ---
    "[INFRA] IT Connect": "https://www.it-connect.fr/feed/",
    "[INFRA] LinuxFR.org": "https://linuxfr.org/news.atom",
    "[INFRA] ZDNet Cloud": "https://www.zdnet.fr/feeds/rss/actualites/cloud-computing/",
    # URL Corrig√©e (l'ancienne √©tait instable)
    "[INFRA] Toolinux": "http://feeds.feedburner.com/toolinux",
    "[INFRA üá∫üá∏] AWS What's New": "https://aws.amazon.com/about-aws/whats-new/recent/feed/",
    
    # --- TECH, IA & DATA ---
    "[IA] Actu IA": "https://www.actuia.com/feed/",
    "[TECH] Next": "https://next.ink/feed/", 
    "[TECH] Korben": "https://korben.info/feed",
    # URL Corrig√©e (fichier XML direct)
    "[IA üá∫üá∏] OpenAI Blog": "https://openai.com/blog/rss.xml",
    "[DATA üá∫üá∏] KDnuggets": "https://www.kdnuggets.com/feed",
    "[SCIENCE] Numerama": "https://www.numerama.com/feed/"
}

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'), 
    'database': os.getenv('DB_NAME')
}

def recuperer_xml(url):
    headers = {'User-Agent': USER_AGENT}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logging.error(f"Erreur connexion {url} : {e}")
        return None

def parser_articles(xml, nom_source):
    soup = BeautifulSoup(xml, 'xml') 
    items = soup.find_all('item')[:100]
    resultats = []
    
    for item in items:
        date_jour = datetime.date.today().strftime('%Y-%m-%d')
        titre = item.title.text[:255] if item.title else "Sans titre"
        lien = item.link.text[:255] if item.link else ""

        if lien:
            resultats.append({
                "source": nom_source,
                "titre": titre,
                "lien": lien,
                "date": date_jour
            })
    return resultats

def sauvegarder_mysql(articles):
    conn = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        ajouts = 0
        
        for art in articles:
            query = "INSERT IGNORE INTO articles (date, source, titre, lien) VALUES (%s, %s, %s, %s)"
            valeurs = (art['date'], art['source'], art['titre'], art['lien'])
            cursor.execute(query, valeurs)
            if cursor.rowcount > 0:
                ajouts += 1
                
        conn.commit()
        logging.info(f"Succ√®s SQL : {ajouts} articles ajout√©s.")
        return ajouts
    except mysql.connector.Error as err:
        console.print(f"[error]‚ùå Erreur MySQL : {err}[/error]")
        logging.error(f"Erreur MySQL : {err}")
        return 0
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

def main():
    logging.info("--- D√âMARRAGE DU SCRAPER ---")
    console.print(Panel.fit("ü§ñ [bold cyan]Scraper de Veille Technologique[/bold cyan]", border_style="blue"))
    
    tous_les_articles = []

    # --- √âTAPE 1 : R√âCUP√âRATION ---
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("{task.percentage:>3.0f}%"),
        console=console
    ) as progress:
        
        task = progress.add_task("[green]R√©cup√©ration des flux...", total=len(SOURCES))
        
        for nom_site, url_rss in SOURCES.items():
            xml = recuperer_xml(url_rss)
            if xml:
                articles_site = parser_articles(xml, nom_site)
                tous_les_articles.extend(articles_site)
            else:
                console.print(f"[warning]‚ö†Ô∏è √âchec sur {nom_site}[/warning]")
                logging.warning(f"√âchec flux : {nom_site}")
            
            progress.advance(task)

    console.print(f"\n[bold]üìä Total r√©cup√©r√© : {len(tous_les_articles)} articles.[/bold]")

    # --- √âTAPE 2 : FILTRAGE ---
    articles_a_sauvegarder = []

    if sys.stdin.isatty():
        # Mode Interactif
        logging.info("Mode : Manuel (Interactif)")
        console.print("\n[bold yellow]üëÄ MODE INTERACTIF - TRI MANUEL[/bold yellow]")
        
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("#", style="dim", width=4)
        table.add_column("Source", style="cyan", width=25)
        table.add_column("Titre", style="white")

        for i, art in enumerate(tous_les_articles):
            table.add_row(str(i+1), art['source'], art['titre'])
        
        console.print(table)
        
        choix = console.input("[bold yellow]‚ùå Num√©ros √† IGNORER (ex: 1,3) ou Entr√©e : [/bold yellow]")
        
        indices_a_ignorer = []
        if choix.strip():
            try:
                indices_a_ignorer = [int(x.strip()) for x in choix.split(',')]
            except ValueError:
                console.print("[error]Saisie invalide, tout est conserv√©.[/error]")

        for i, art in enumerate(tous_les_articles):
            if (i + 1) not in indices_a_ignorer:
                articles_a_sauvegarder.append(art)
    else:
        # Mode Automatique
        logging.info("Mode : Automatique (Cron/Arri√®re-plan)")
        console.print("[dim]ü§ñ Mode automatique : Sauvegarde compl√®te.[/dim]")
        articles_a_sauvegarder = tous_les_articles

    # --- √âTAPE 3 : SAUVEGARDE ---
    if articles_a_sauvegarder:
        with console.status("[bold green]Sauvegarde en base de donn√©es...[/bold green]"):
            nb_ajouts = sauvegarder_mysql(articles_a_sauvegarder)
        
        console.print(Panel(f"‚úÖ TERMIN√â\n[bold green]{nb_ajouts} nouveaux articles ajout√©s[/bold green]", border_style="green"))
    else:
        console.print("[warning]Aucun article √† sauvegarder.[/warning]")
        logging.info("Aucun article sauvegard√©.")

    logging.info("--- FIN DU SCRAPER ---\n")

if __name__ == "__main__":
    main()