"""
PROJET : Web Scrapper de Veille Technologique AutomatisÃ©e
Auteur : LÃ©o Dupont - BTS SIO 1
Description : 
Ce script permet de rÃ©cupÃ©rer des articles depuis plusieurs flux RSS (Cyber et Dev).
Les articles sont filtrÃ©s (manuel ou auto) puis stockÃ©s dans une base de donnÃ©es MySQL.
IdÃ©al pour alimenter un tableau de bord de veille.
"""

import os
import requests
from bs4 import BeautifulSoup
import datetime
import mysql.connector
import sys
from dotenv import load_dotenv

# --- 1. CHARGEMENT DES VARIABLES D'ENVIRONNEMENT ---
# On utilise load_dotenv pour ne pas Ã©crire les identifiants en dur dans le code
# C'est une bonne pratique de sÃ©curitÃ© vue en cours de CybersÃ©curitÃ©
load_dotenv()

# --- 2. CONFIGURATION DES SOURCES ---
# J'ai choisi des sources mixtes pour couvrir les deux options du BTS (SLAM et SISR)
SOURCES = {
    "[CYBER] ANSSI (CERT-FR)": "https://www.cert.ssi.gouv.fr/feed/",
    "[CYBER] Le Monde Informatique": "https://www.lemondeinformatique.fr/flux-rss/rubrique/cybersecurite/rss.xml",
    "[CYBER] Zataz": "https://www.zataz.com/feed/",
    "[DEV] Developpez.com": "https://www.developpez.com/index/rss",
    "[DEV] Human Coders": "https://news.humancoders.com/items/feed"
}

# On dÃ©finit un User-Agent pour simuler un navigateur et Ã©viter d'Ãªtre bloquÃ© par les sites
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

# Configuration de la connexion MySQL via les variables du fichier .env
DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'), 
    'database': os.getenv('DB_NAME')
}

# --- 3. FONCTION DE RÃ‰CUPÃ‰RATION (REQUÃŠTE HTTP) ---
def recuperer_xml(url):
    """
    Utilise la bibliothÃ¨que 'requests' pour tÃ©lÃ©charger le contenu du flux RSS.
    On gÃ¨re les exceptions avec un bloc try/except pour Ã©viter que le script plante.
    """
    headers = {'User-Agent': USER_AGENT}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        # On vÃ©rifie si la requÃªte a rÃ©ussi (code 200)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"   âŒ Erreur de connexion : {e}")
        return None

# --- 4. FONCTION DE PARSING (EXTRACTION DES DONNÃ‰ES) ---
def parser_articles(xml, nom_source):
    """
    Analyse le XML avec BeautifulSoup. 
    On limite Ã  10 articles par source pour ne pas surcharger la base de donnÃ©es.
    """
    soup = BeautifulSoup(xml, 'xml') 
    
    # On utilise le slicing Python [:10] pour limiter la liste
    items = soup.find_all('item')[:10]
    
    resultats = []
    
    for item in items:
        # On rÃ©cupÃ¨re la date du jour au format SQL (YYYY-MM-DD)
        date_jour = datetime.date.today().strftime('%Y-%m-%d')
        # On limite la taille des chaÃ®nes pour correspondre aux colonnes VARCHAR de la BDD
        titre = item.title.text[:255] if item.title else "Sans titre"
        lien = item.link.text[:255] if item.link else ""

        if lien:
            # On stocke les infos dans un dictionnaire avant de les ajouter Ã  la liste
            resultats.append({
                "source": nom_source,
                "titre": titre,
                "lien": lien,
                "date": date_jour
            })
    return resultats

# --- 5. FONCTION DE STOCKAGE (BASE DE DONNÃ‰ES) ---
def sauvegarder_mysql(articles):
    """
    Se connecte Ã  MySQL et insÃ¨re les articles.
    On utilise 'INSERT IGNORE' pour ne pas avoir de doublons si on lance le script plusieurs fois.
    """
    conn = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        ajouts = 0
        
        for art in articles:
            # RequÃªte prÃ©parÃ©e pour Ã©viter les injections SQL
            query = """
                INSERT IGNORE INTO articles (date, source, titre, lien) 
                VALUES (%s, %s, %s, %s)
            """
            valeurs = (art['date'], art['source'], art['titre'], art['lien'])
            
            cursor.execute(query, valeurs)
            # On incrÃ©mente le compteur si une ligne a bien Ã©tÃ© ajoutÃ©e
            if cursor.rowcount > 0:
                ajouts += 1
                
        # On valide la transaction
        conn.commit()
        return ajouts
        
    except mysql.connector.Error as err:
        print(f"   âŒ Erreur MySQL : {err}")
        return 0
    finally:
        # On ferme proprement le curseur et la connexion (vu en SQL)
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

# --- 6. PROGRAMME PRINCIPAL ---
def main():
    print(f"ğŸ¤– Lancement de la Veille...")
    print(f"ğŸ“… Date : {datetime.datetime.now()}")
    print("-" * 50)
    
    tous_les_articles = []

    # ETAPE A : RÃ‰CUPÃ‰RATION
    # On boucle sur le dictionnaire des sources dÃ©fini au dÃ©but
    for nom_site, url_rss in SOURCES.items():
        print(f"ğŸŒ {nom_site}...", end=" ")
        xml = recuperer_xml(url_rss)
        
        if xml:
            articles_site = parser_articles(xml, nom_site)
            tous_les_articles.extend(articles_site)
            print(f"âœ… OK ({len(articles_site)} articles)")
        else:
            print("âš ï¸ Erreur")

    print("-" * 50)
    print(f"ğŸ“Š TOTAL RÃ‰CUPÃ‰RÃ‰ : {len(tous_les_articles)} articles.")

    # ETAPE B : FILTRAGE (Intervention humaine ou Automatique)
    articles_a_sauvegarder = []

    # sys.stdin.isatty() permet de savoir si on lance le script Ã  la main (True)
    # ou via une tÃ¢che planifiÃ©e type Cron (False).
    if sys.stdin.isatty():
        print("\nğŸ‘€ MODE MANUEL DÃ‰TECTÃ‰ - VÃ©rification des articles :")
        
        # Affichage pour l'utilisateur
        for i, art in enumerate(tous_les_articles):
            print(f"[{i+1}] [{art['source']}] {art['titre']}")
        
        print("\n" + "-"*30)
        choix = input("âŒ Entrez les numÃ©ros Ã  IGNORER (ex: 1, 3) ou EntrÃ©e pour tout garder : ")
        
        indices_a_ignorer = []
        if choix.strip():
            try:
                # ComprÃ©hension de liste pour convertir la saisie en entiers
                indices_a_ignorer = [int(x.strip()) for x in choix.split(',')]
            except ValueError:
                print("âš ï¸ Erreur de saisie. Tout sera conservÃ©.")

        # On filtre la liste globale
        for i, art in enumerate(tous_les_articles):
            if (i + 1) not in indices_a_ignorer:
                articles_a_sauvegarder.append(art)
            else:
                print(f"ğŸ—‘ï¸ IgnorÃ© : {art['titre']}")
                
    else:
        # En mode automatique (Cron), on ne peut pas poser de question
        print("\nğŸ¤– MODE AUTOMATIQUE DÃ‰TECTÃ‰ : Sauvegarde intÃ©grale sans interaction.")
        articles_a_sauvegarder = tous_les_articles

    # ETAPE C : SAUVEGARDE FINALE
    if articles_a_sauvegarder:
        print(f"\nğŸ’¾ Enregistrement de {len(articles_a_sauvegarder)} articles en base...")
        nb_ajouts = sauvegarder_mysql(articles_a_sauvegarder)
        print(f"âœ… TERMINÃ‰ : {nb_ajouts} nouveaux articles ajoutÃ©s dans MySQL.")
    else:
        print("âŒ Aucun article Ã  sauvegarder.")

# Point d'entrÃ©e standard du script
if __name__ == "__main__":
    main()
